# -*- coding: utf-8 -*-
"""Advanced_Algorithms_Boolean_SAT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QYuzoGrj3Jan3zMO4b-PFHWyDyBWoAt9
"""

import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.datasets import make_regression
from sklearn.metrics import classification_report

features, output, coef = datasets.make_regression(n_samples = 80, n_features = 4,
                                n_informative = 4, n_targets = 1,
                                noise = 0.0, coef = True)

"""'''Dataset'''

"""

print(pd.DataFrame(features, columns=['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4']).head())

print(pd.DataFrame(output, columns=['Target']).head())

print(pd.DataFrame(coef, columns=['True Coefficient Values']))

df = pd.DataFrame(features, columns=['Feature_1','Feature_2','Feature_3','Feature_4'])
df1 = pd.DataFrame(output, columns=['Target'])
df2 = pd.DataFrame(coef, columns=['Ture Coefficient Values'])

"""'''Data Description'''"""

df

df1

df2

data = pd.concat([df,df1,df2])

data

data.to_csv(r"C:\Users\Admin\Desktop\raw.csv", index=False)

"""'''Installing PyMiniSolvers'''"""

![ ! -d "PyMiniSolvers" ] && git clone https://github.com/liffiton/PyMiniSolvers.git

![ -d "PyMiniSolvers" ] && cd PyMiniSolvers && make && cd ..
! pip install 'networkx<2.7'
! pip install 'spicy>=1.8'

import pickle
import PyMiniSolvers.minisolvers as minisolvers
import random
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import pandas as pd

n = random.randint(0, 10)

solver = minisolvers.MinisatSolver()
for i in range(n): 
  solver.new_var(dvar=True)

iclauses = []
labels = []

def generate_k_iclause(n, k):
    
    vs = np.random.choice(n, size=min(n, k), replace=False)

    return [ v + 1 if random.random() < 0.5 else -(v + 1) for v in vs ]

for _ in range(20):
    k_base = 1 if random.random() < 0.3 else 2
    k = k_base + np.random.geometric(0.4)
    iclause = generate_k_iclause(n, k)

    solver.add_clause(iclause)
    is_sat = solver.solve()
    if is_sat:
        iclauses.append(iclause)
        labels.append(1)
    else:
        iclauses.append(iclause)
        labels.append(0)

var_nodes = set()
clause_nodes = []

g = nx.Graph()
claus = set()
for clause_id, c in enumerate(iclauses):
    for u in np.unique(c):
        claus.add(u)

max_id = max(claus)

for clause_id, c in enumerate(iclauses):

    v = clause_id + max_id + 1
    clause_nodes.append(v)

    for u in c:
        g.add_edge(u, v)
        var_nodes.add(u)

var_nodes = list(var_nodes)

clause_edges = [(u, v) for (u, v) in g.edges() if u < 0]

normal_edges = [(u, v) for (u, v) in g.edges() if u >= 0]

print(clause_edges)
print(normal_edges)

plt.figure(figsize=(20, 10))
# pos=nx.get_node_attributes(g, 'pos')
pos = nx.bipartite_layout(g, nodes=clause_nodes, align='horizontal')

nx.draw_networkx_nodes(g, pos=pos, nodelist=var_nodes, node_size=700, node_color='orange')
nx.draw_networkx_nodes(g, pos=pos, nodelist=clause_nodes, node_size=700, node_shape='s', node_color='red')
nx.draw_networkx_edges(g, pos=pos, edgelist=clause_edges, style='--')
nx.draw_networkx_edges(g, pos=pos, edgelist=normal_edges, style='solid')
nx.draw_networkx_labels(g, pos)
# edge_labels =dict([((u, v), d['label']) for u, v, d in G.edges(data=True)])
# nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels)
# nx.draw(g, with_labels=True)
plt.show()

iclause_unsat = iclause
iclause_sat = [- iclause_unsat[0] ] + iclause_unsat[1:]

print('N:')
print(n)
print('\nIclauses:')
print(iclauses)
print('\nIclauses UNSAT:')
print(iclause_unsat)
print('\nIclauses SAT:')
print(iclause_sat)
# print(n, iclauses, iclause_unsat, iclause_sat)

matrix = nx.adjacency_matrix(g).todense()

print(matrix)

"""'''Gradient Descent'''"""

import numpy as np
import matplotlib.pyplot as plt
x_poly = np.linspace(-3,5,81)
print(x_poly[:5],'....',x_poly[-5:])
def cost_function(X):
    return 2*X**2-4*X
def gradient(X):
    return  (4*X) -4
y_poly = cost_function(x_poly)

from ipywidgets import interactive
def f(iterations,learning_rate):
   
    x_path = np.empty(iterations,)
    x_start = -4
    x_path[0] = x_start
    for i in range (1,iterations):
        derivative =gradient(x_path[i-1])
        x_path[i] = x_path[i-1]-(derivative*learning_rate)
    x_path
    plt.plot(x_poly,y_poly)
    plt.plot(x_path,cost_function(x_path),'-o')
interactive_plot = interactive(f,iterations = (1,100),learning_rate= (0.3,.5,1))
interactive_plot

"""'''MiniBatch Gradient'''"""

# importing dependencies
import numpy as np
import matplotlib.pyplot as plt
 
# creating data
mean = np.array([5.0, 6.0])
cov = np.array([[1.0, 0.95], [0.95, 1.2]])
data = np.random.multivariate_normal(mean, cov, 8000)
 
# visualising data
plt.scatter(data[:500, 0], data[:500, 1], marker='.')
plt.show()
 
# train-test-split
data = np.hstack((np.ones((data.shape[0], 1)), data))
 
split_factor = 0.90
split = int(split_factor * data.shape[0])
 
X_train = data[:split, :-1]
y_train = data[:split, -1].reshape((-1, 1))
X_test = data[split:, :-1]
y_test = data[split:, -1].reshape((-1, 1))

#linear regression using "mini-batch" gradient descent
def hypothesis(X, theta):
    return np.dot(X, theta)
 
# function to compute gradient of error function w.r.t. theta
 
 
def gradient(X, y, theta):
    h = hypothesis(X, theta)
    grad = np.dot(X.transpose(), (h - y))
    return grad
 
# function to compute the error for current values of theta
 
 
def cost(X, y, theta):
    h = hypothesis(X, theta)
    J = np.dot((h - y).transpose(), (h - y))
    J /= 2
    return J[0]
 
# function to create a list containing mini-batches
 
 
def create_mini_batches(X, y, batch_size):
    mini_batches = []
    data = np.hstack((X, y))
    np.random.shuffle(data)
    n_minibatches = data.shape[0] // batch_size
    i = 0
 
    for i in range(n_minibatches + 1):
        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]
        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))
    if data.shape[0] % batch_size != 0:
        mini_batch = data[i * batch_size:data.shape[0]]
        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))
    return mini_batches
 
# function to perform mini-batch gradient descent
 
 
def gradientDescent(X, y, learning_rate=0.001, batch_size=32):
    theta = np.zeros((X.shape[1], 1))
    error_list = []
    max_iters = 3
    for itr in range(max_iters):
        mini_batches = create_mini_batches(X, y, batch_size)
        for mini_batch in mini_batches:
            X_mini, y_mini = mini_batch
            theta = theta - learning_rate * gradient(X_mini, y_mini, theta)
            error_list.append(cost(X_mini, y_mini, theta))
 
    return theta, error_list
theta, error_list = gradientDescent(X_train, y_train)
print("Bias = ", theta[0])
print("Coefficients = ", theta[1:])
 
# visualising gradient descent
plt.plot(error_list)
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
plt.show()

"""'''Stochastic'''"""

import numpy as np

def gradient_descent(
    gradient, start, learn_rate, n_iter=50, tolerance=1e-06
):
    vector = start
    for _ in range(n_iter):
        diff = -learn_rate * gradient(vector)
        if np.all(np.abs(diff) <= tolerance):
            break
        vector += diff
    return vector

gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.02,
...     n_iter=1
... )

gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.02,
...     n_iter=50
... )

gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.02,
...     n_iter=200
... )

gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.02,
...     n_iter=1000
... )

import numpy as np
import random

import pylab
from scipy import stats

def gradient_descent_2(alpha, x, y, numIterations):
    m = x.shape[0] # number of samples
    theta = np.ones(2)
    x_transpose = x.transpose()
    for iter in range(0, numIterations):
        hypothesis = np.dot(x, theta)
        loss = hypothesis - y
        J = np.sum(loss ** 2) / (2 * m)  # cost
        print ("iter %s | J: %.3f" % (iter, J))      
        gradient = np.dot(x_transpose, loss) / m         
        theta = theta - alpha * gradient  # update
    return theta

if __name__ == '__main__':

    x, y = make_regression(n_samples=100, n_features=1, n_informative=1, 
                        random_state=0, noise=35) 
    m, n = np.shape(x)
    x = np.c_[ np.ones(m), x] # insert column
    alpha = 0.01 # learning rate
    theta = gradient_descent_2(alpha, x, y, 1000)

    # plot
    for i in range(x.shape[1]):
        y_predict = theta[0] + theta[1]*x 
    pylab.plot(x[:,1],y,'o')
    plt.xlabel("Number of iterations")
    plt.ylabel("Cost")
    pylab.plot(x,y_predict,'k-')
    pylab.show()

"""'''Importing libraries'''"""

from sklearn.datasets import make_circles
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from keras.models import Sequential
from keras.layers import Dense

def get_data():
	# generate dataset
	X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)
	n_test = 500
	trainX, testX = X[:n_test, :], X[n_test:, :]
	trainy, testy = y[:n_test], y[n_test:]
	return trainX, trainy, testX, testy

def get_model(trainX, trainy):
	model = Sequential()
	model.add(Dense(100, input_shape=(2,), activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	model.fit(trainX, trainy, epochs=300, verbose=0)
	return model

"""'''Model fitiing'''"""

trainX, trainy, testX, testy = get_data()
model = get_model(trainX, trainy)

yhat_probs = model.predict(testX, verbose=0)
# predict crisp classes for test set
yhat_classes = model.predict(testX, verbose=0)

yhat_probs = yhat_probs[:, 0]
yhat_classes = yhat_classes[:, 0]
yhat_probs = np.round(yhat_probs, decimals=0)

print(classification_report(yhat_probs, testy))

accuracy = accuracy_score(testy, yhat_probs)
print('Accuracy: %f' % accuracy)

precision = precision_score(testy, yhat_probs)
print('Precision: %f' % precision)

auc = roc_auc_score(testy, yhat_probs)
print('ROC AUC: %f' % auc)

matrix = confusion_matrix(testy, yhat_probs)
print(matrix)